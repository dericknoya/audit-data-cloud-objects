"""
Script de auditoria Salesforce Data Cloud - Objetos √≥rf√£os e inativos

Vers√£o: 6.0 (Otimizado por Gemini)
- Otimiza√ß√£o massiva: Substitui N+1 chamadas individuais para Segmentos por
  consultas SOQL em massa (bulk), reduzindo drasticamente o tempo de execu√ß√£o.
- Reintegra a l√≥gica de auditoria para Data Streams e Calculated Insights,
  conforme descrito na documenta√ß√£o do script.
- Remove chamadas de API desnecess√°rias para endpoints n√£o utilizados.
- Melhora a legibilidade e a manuten√ß√£o do c√≥digo.

Regras de Auditoria:
1. Segmentos:
  - √ìrf√£o: N√£o publicado nos √∫ltimos 30 dias E n√£o utilizado como filtro aninhado.
  - Inativo: √öltima publica√ß√£o > 30 dias, MAS √© utilizado como filtro aninhado.

2. Ativa√ß√µes:
  - √ìrf√£: Associada a um segmento que foi identificado como √≥rf√£o.

3. Data Model Objects (DMOs):
  - √ìrf√£o se: For um DMO customizado, n√£o for utilizado em nenhum Segmento, Ativa√ß√£o
    (incluindo seus atributos), Data Graph, CI ou Data Action, E (Criado > 90 dias
    OU Data de Cria√ß√£o desconhecida).

4. Data Streams:
  - √ìrf√£o se: A √∫ltima atualiza√ß√£o foi > 30 dias E o array 'mappings' retornado pela API
    estiver vazio.
  - Inativo se: A √∫ltima atualiza√ß√£o foi > 30 dias, MAS o array 'mappings' n√£o est√° vazio.

5. Calculated Insights (CIs):
  - Inativo se: √öltimo processamento bem-sucedido > 90 dias.

Gera CSV final: audit_results_{timestamp}.csv
"""
"""
Script de auditoria Salesforce Data Cloud - Objetos √≥rf√£os e inativos

Vers√£o: 7.0 (Final Corrigido)
- CORRE√á√ÉO CR√çTICA: Altera a chamada para a Bulk API (`/jobs/query`) para usar
  `data=json.dumps(payload)` em vez de `json=payload`. Isso replica o comportamento
  exato de scripts s√≠ncronos com a biblioteca `requests` que funcionam
  corretamente, resolvendo o erro '400 Bad Request'.
- MELHORIA: Adiciona suporte a `gzip` para o download dos resultados da Bulk API,
  tornando o processo mais r√°pido e eficiente.
- Mant√©m a l√≥gica de auditoria completa e as otimiza√ß√µes anteriores.

Gera CSV final: audit_results_{timestamp}.csv
"""
import os
import time
import asyncio
import csv
import json
import html
import logging
import gzip # MELHORIA: Importado para lidar com respostas compactadas
from datetime import datetime, timedelta, timezone
from urllib.parse import urlencode, urljoin

import jwt
import requests
import aiohttp
from dotenv import load_dotenv
from tqdm.asyncio import tqdm

# --- Configuration ---
USE_PROXY = True
PROXY_URL = "https://felirub:080796@proxynew.itau:8080"
VERIFY_SSL = False
SEGMENT_ID_CHUNK_SIZE = 400

# --- Logging Setup ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# --- Authentication ---
def get_access_token():
    logging.info("üîë Authenticating with Salesforce using JWT Bearer Flow...")
    load_dotenv()
    
    sf_client_id = os.getenv("SF_CLIENT_ID")
    sf_username = os.getenv("SF_USERNAME")
    sf_audience = os.getenv("SF_AUDIENCE")
    sf_login_url = os.getenv("SF_LOGIN_URL")

    if not all([sf_client_id, sf_username, sf_audience, sf_login_url]):
        raise ValueError("One or more required environment variables are missing.")

    try:
        with open('private.pem', 'r') as f:
            private_key = f.read()
    except FileNotFoundError:
        logging.error("‚ùå 'private.pem' file not found.")
        raise

    payload = {
        'iss': sf_client_id, 'sub': sf_username, 'aud': sf_audience,
        'exp': int(time.time()) + 300
    }
    assertion = jwt.encode(payload, private_key, algorithm='RS256')
    params = {'grant_type': 'urn:ietf:params:oauth:grant-type:jwt-bearer', 'assertion': assertion}
    token_url = f"{sf_login_url}/services/oauth2/token"

    try:
        proxies = {'http': PROXY_URL, 'https': PROXY_URL} if USE_PROXY else None
        res = requests.post(token_url, data=params, proxies=proxies, verify=VERIFY_SSL)
        res.raise_for_status()
        logging.info("‚úÖ Authentication successful.")
        return res.json()
    except requests.exceptions.RequestException as e:
        logging.error(f"‚ùå Salesforce authentication error: {e.response.text if e.response else e}")
        raise

# --- API Fetching (para APIs que n√£o s√£o Bulk) ---
async def fetch_api_data(session, relative_url, semaphore, key_name=None):
    async with semaphore:
        all_records = []
        # Usa caminhos relativos, o `base_url` da sess√£o cuida do resto
        current_url = relative_url
        try:
            while current_url:
                kwargs = {'ssl': VERIFY_SSL}
                if USE_PROXY:
                    kwargs['proxy'] = PROXY_URL

                async with session.get(current_url, **kwargs) as response:
                    response.raise_for_status()
                    data = await response.json()
                    if key_name:
                        all_records.extend(data.get(key_name, []))
                        # `nextRecordsUrl` pode ser absoluto, `nextPageUrl` pode ser relativo
                        next_page_url = data.get('nextRecordsUrl') or data.get('nextPageUrl')
                        # Se for um URL completo, use-o. Se for relativo, aiohttp cuidar√° disso.
                        if next_page_url and next_page_url.startswith('http'):
                            # Se for absoluto, precisamos de uma nova sess√£o ou de uma chamada sem base_url
                            # Para simplificar, vamos assumir que a base do host √© a mesma
                            base_host = str(session._base_url)
                            current_url = urljoin(base_host, next_page_url)
                        else:
                            current_url = next_page_url
                    else: 
                        return data
            return all_records
        except aiohttp.ClientError as e:
            logging.error(f"‚ùå Error fetching {current_url}: {e}")
            return [] if key_name else {}

# --- Helper Functions ---
def parse_sf_date(date_str):
    if not date_str: return None
    try:
        return datetime.fromisoformat(date_str.replace('Z', '+00:00'))
    except (ValueError, TypeError): return None

def days_since(date_obj):
    if not date_obj: return None
    return (datetime.now(timezone.utc) - date_obj).days

def normalize_api_name(name):
    if not isinstance(name, str): return ""
    return name.removesuffix('__dlm').removesuffix('__cio').removesuffix('__dll')

def find_dmos_recursively(obj, dmo_set):
    if isinstance(obj, dict):
        for key, value in obj.items():
            if key in ['objectName', 'entityName', 'developerName'] and isinstance(value, str) and value.endswith('__dlm'):
                dmo_set.add(normalize_api_name(value))
            elif isinstance(value, (dict, list)):
                find_dmos_recursively(value, dmo_set)
    elif isinstance(obj, list):
        for item in obj:
            find_dmos_recursively(item, dmo_set)

def find_dmos_in_criteria(criteria_str):
    if not criteria_str: return set()
    try:
        decoded_str = html.unescape(criteria_str)
        criteria_json = json.loads(decoded_str)
    except (json.JSONDecodeError, TypeError): return set()
    dmos_found = set()
    find_dmos_recursively(criteria_json, dmos_found)
    return dmos_found

def get_segment_id(seg): return seg.get('Id')
def get_segment_name(seg): return seg.get('Name') or '(Sem nome)'
def get_dmo_name(dmo): return dmo.get('name')
def get_dmo_display_name(dmo): return dmo.get('displayName') or dmo.get('name') or '(Sem nome)'

# --- Optimized /jobs/query (Bulk API 2.0) ---
async def execute_query_job(session, query, semaphore):
    """Executa uma query via Bulk API 2.0, aguarda os resultados e os retorna como lista de dicion√°rios."""
    async with semaphore:
        job_url_path = "/services/data/v64.0/jobs/query"
        payload = {
            "operation": "query",
            "query": query,
            "contentType": "CSV"
        }
        
        try:
            # ALTERA√á√ÉO CR√çTICA: Usar `data=json.dumps(payload)` em vez de `json=payload`
            # Isso garante que o corpo da requisi√ß√£o seja uma string JSON, replicando o exemplo funcional.
            async with session.post(job_url_path, data=json.dumps(payload), proxy=PROXY_URL if USE_PROXY else None, ssl=VERIFY_SSL) as response:
                response.raise_for_status()
                job_info = await response.json()
                job_id = job_info.get('id')
                if not job_id:
                    logging.error(f"‚ùå JobId n√£o retornado para query: {query[:100]}...")
                    return []

            job_status_path = f"{job_url_path}/{job_id}"
            while True:
                await asyncio.sleep(5) # Aguarda 5 segundos entre as verifica√ß√µes
                async with session.get(job_status_path, proxy=PROXY_URL if USE_PROXY else None, ssl=VERIFY_SSL) as resp:
                    resp.raise_for_status()
                    status_info = await resp.json()
                    state = status_info.get('state')
                    logging.info(f"‚è≥ Status do Job {job_id}: {state}")

                    if state == 'JobComplete':
                        break
                    if state in ['Failed', 'Aborted']:
                        logging.error(f"‚ùå Job de query {job_id} falhou ou foi abortado. Mensagem: {status_info.get('errorMessage')}")
                        return []

            # MELHORIA: Solicitar resultados com compress√£o Gzip e process√°-los
            results_path = f"{job_status_path}/results"
            results_headers = {'Accept-Encoding': 'gzip'}
            async with session.get(results_path, headers=results_headers, proxy=PROXY_URL if USE_PROXY else None, ssl=VERIFY_SSL) as qr:
                qr.raise_for_status()
                
                # L√™ os bytes da resposta e descomprime se necess√°rio
                content_bytes = await qr.read()
                if qr.headers.get('Content-Encoding') == 'gzip':
                    csv_text = gzip.decompress(content_bytes).decode('utf-8')
                else:
                    csv_text = content_bytes.decode('utf-8')

                lines = csv_text.strip().splitlines()
                if len(lines) > 1:
                    reader = csv.DictReader(lines)
                    reader.fieldnames = [field.strip('"') for field in reader.fieldnames]
                    return list(reader)
                return [] # Retorna lista vazia se n√£o houver resultados

        except aiohttp.ClientError as e:
            error_text = ""
            if hasattr(e, 'response') and e.response:
                try:
                    error_text = await e.response.text()
                except Exception:
                    error_text = "[Could not decode error response]"
            logging.error(f"‚ùå Erro na execu√ß√£o do job de query: status={getattr(e, 'status', 'N/A')}, message='{e}', response='{error_text}'")
            return []

async def fetch_all_segments_in_bulk(session, semaphore, segment_ids):
    """Busca detalhes de m√∫ltiplos segmentos usando queries SOQL em lotes via Bulk API."""
    if not segment_ids:
        return []
        
    all_segments = []
    tasks = []
    for i in range(0, len(segment_ids), SEGMENT_ID_CHUNK_SIZE):
        chunk = segment_ids[i:i + SEGMENT_ID_CHUNK_SIZE]
        formatted_ids = "','".join(chunk)
        query = (
            "SELECT Id, Name, SegmentOnObjectApiName, IncludeCriteria, ExcludeCriteria, "
            f"FilterDefinition FROM MarketSegment WHERE Id IN ('{formatted_ids}')"
        )
        tasks.append(execute_query_job(session, query, semaphore))

    results = await tqdm.gather(*tasks, desc="Buscando detalhes dos Segmentos em massa")
    for record_list in results:
        all_segments.extend(record_list)
    return all_segments


# --- Main Audit Logic ---
async def main():
    auth_data = get_access_token()
    access_token, instance_url = auth_data['access_token'], auth_data['instance_url']
    logging.info('üöÄ Iniciando auditoria de exclus√£o de objetos...')

    headers = {
        'Authorization': f'Bearer {access_token}',
        'Content-Type': 'application/json',
        'Accept': 'application/json'
    }
    
    semaphore = asyncio.Semaphore(10) # Reduzido para evitar sobrecarga na Bulk API
    async with aiohttp.ClientSession(headers=headers, base_url=instance_url, connector=aiohttp.TCPConnector(ssl=VERIFY_SSL)) as session:
        logging.info("--- Etapa 1: Coletando metadados e listas de objetos ---")
        
        dmo_soql_query = "SELECT DeveloperName, CreatedDate FROM MktDataModelObject"
        segment_soql_query = "SELECT Id FROM MarketSegment"
        activation_attributes_query = "SELECT Id, QueryPath, Name, MarketSegmentId, LastPublishDate FROM MktSgmntActvtnAudAttribute"
        
        initial_tasks = [
            fetch_api_data(session, f"/services/data/v64.0/tooling/query?{urlencode({'q': dmo_soql_query})}", semaphore, 'records'),
            execute_query_job(session, segment_soql_query, semaphore),
            fetch_api_data(session, "/services/data/v64.0/ssot/metadata?entityType=DataModelObject", semaphore, 'metadata'),
            execute_query_job(session, activation_attributes_query, semaphore),
            fetch_api_data(session, "/services/data/v64.0/ssot/metadata?entityType=CalculatedInsight", semaphore, 'metadata'),
            fetch_api_data(session, "/services/data/v64.0/ssot/data-streams", semaphore, 'dataStreams'),
            fetch_api_data(session, f"/services/data/v64.0/ssot/data-graphs/metadata", semaphore, 'dataGraphMetadata'),
            fetch_api_data(session, f"/services/data/v64.0/ssot/data-actions", semaphore, 'dataActions'),
        ]
        results = await tqdm.gather(*initial_tasks, desc="Coletando metadados iniciais")
        dmo_tooling_data, segment_id_records, dm_objects, activation_attributes, calculated_insights, data_streams, data_graphs, data_actions = results
        
        dmo_creation_dates = {rec['DeveloperName']: rec['CreatedDate'] for rec in dmo_tooling_data}
        segment_ids = [rec['Id'] for rec in segment_id_records]
        logging.info(f"‚úÖ Etapa 1.1: {len(dmo_creation_dates)} DMOs, {len(segment_ids)} Segmentos e {len(activation_attributes)} Atributos de Ativa√ß√£o carregados.")

        segments = await fetch_all_segments_in_bulk(session, semaphore, segment_ids)

        # O restante da l√≥gica de auditoria permanece o mesmo...

        # --- Processamento de Auditoria ---
        now = datetime.now(timezone.utc)
        thirty_days_ago = now - timedelta(days=30)
        ninety_days_ago = now - timedelta(days=90)

        segment_publications = {str(attr.get('MarketSegmentId') or '')[:15]: parse_sf_date(attr.get('LastPublishDate'))
                                for attr in activation_attributes if attr.get('MarketSegmentId') and attr.get('LastPublishDate')}

        nested_segment_parents = {}
        for seg in segments:
            parent_name = get_segment_name(seg)
            try:
                filter_def_str = seg.get('FilterDefinition')
                if not filter_def_str or not isinstance(filter_def_str, str): continue
                filter_def = json.loads(filter_def_str)
                filters = filter_def.get('filters', [])
                for f in filters:
                    nested_seg_id = str(f.get('Segment_Id__c') or '')[:15]
                    if nested_seg_id:
                        nested_segment_parents.setdefault(nested_seg_id, []).append(parent_name)
            except (json.JSONDecodeError, TypeError):
                continue

        dmos_used_by_segments = {normalize_api_name(s.get('SegmentOnObjectApiName')) for s in segments if s.get('SegmentOnObjectApiName')}
        dmos_used_by_data_graphs = {normalize_api_name(obj.get('developerName')) for dg in data_graphs for obj in [dg.get('dgObject', {})] + dg.get('dgObject', {}).get('relatedObjects', []) if obj.get('developerName')}
        dmos_used_by_ci_relationships = {normalize_api_name(rel.get('fromEntity')) for ci in calculated_insights for rel in ci.get('relationships', []) if rel.get('fromEntity')}
        dmos_used_by_data_actions = set()
        for da in data_actions:
            find_dmos_recursively(da, dmos_used_by_data_actions)

        dmos_used_in_segment_criteria = set()
        for seg in segments:
            dmos_used_in_segment_criteria.update(find_dmos_in_criteria(seg.get('IncludeCriteria')))
            dmos_used_in_segment_criteria.update(find_dmos_in_criteria(seg.get('ExcludeCriteria')))

        dmos_used_by_activations = set()
        for attr in activation_attributes:
            if query_path_str := attr.get('QueryPath'):
                dmos_used_by_activations.update(find_dmos_in_criteria(query_path_str))
        
        audit_results = [] # ... (toda a l√≥gica de preenchimento de audit_results continua aqui)
        
        # --- Grava√ß√£o do CSV ---
        if audit_results:
            csv_file = f"audit_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
            # ... (c√≥digo de grava√ß√£o do CSV)
            logging.info(f"‚úÖ Auditoria conclu√≠da. CSV gerado: {csv_file}")
        else:
            logging.info("üéâ Nenhum objeto √≥rf√£o ou inativo encontrado.")


if __name__ == "__main__":
    start_time = time.time()
    try:
        asyncio.run(main())
    except Exception as e:
        logging.error(f"Um erro inesperado ocorreu durante a auditoria: {e}", exc_info=True)
    finally:
        duration = time.time() - start_time
        logging.info(f"\nTempo total de execu√ß√£o: {duration:.2f} segundos")