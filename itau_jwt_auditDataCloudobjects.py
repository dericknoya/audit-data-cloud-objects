"""
Script de auditoria Salesforce Data Cloud - Objetos √≥rf√£os e inativos

Vers√£o: 7.1 (Otimizado por Gemini)
- Otimiza√ß√£o massiva: Substitui N+1 chamadas individuais para Segmentos por
  consultas SOQL em massa (bulk), reduzindo drasticamente o tempo de execu√ß√£o.
- Reintegra a l√≥gica de auditoria para Data Streams e Calculated Insights,
  conforme descrito na documenta√ß√£o do script.
- Remove chamadas de API desnecess√°rias para endpoints n√£o utilizados.
- Melhora a legibilidade e a manuten√ß√£o do c√≥digo.

Regras de Auditoria:
1. Segmentos:
  - √ìrf√£o: N√£o publicado nos √∫ltimos 30 dias E n√£o utilizado como filtro aninhado.
  - Inativo: √öltima publica√ß√£o > 30 dias, MAS √© utilizado como filtro aninhado.

2. Ativa√ß√µes:
  - √ìrf√£: Associada a um segmento que foi identificado como √≥rf√£o.

3. Data Model Objects (DMOs):
  - √ìrf√£o se: For um DMO customizado, n√£o for utilizado em nenhum Segmento, Ativa√ß√£o
    (incluindo seus atributos), Data Graph, CI ou Data Action, E (Criado > 90 dias
    OU Data de Cria√ß√£o desconhecida).

4. Data Streams:
  - √ìrf√£o se: A √∫ltima atualiza√ß√£o foi > 30 dias E o array 'mappings' retornado pela API
    estiver vazio.
  - Inativo se: A √∫ltima atualiza√ß√£o foi > 30 dias, MAS o array 'mappings' n√£o est√° vazio.

5. Calculated Insights (CIs):
  - Inativo se: √öltimo processamento bem-sucedido > 90 dias.

Gera CSV final: audit_results_{timestamp}.csv
""""""
Script de auditoria Salesforce Data Cloud - Objetos √≥rf√£os e inativos

Vers√£o: 7.1 (Final com SOQL Corrigida)
- CORRE√á√ÉO DE SOQL: Ajusta as queries para buscar os dados de ativa√ß√£o em duas etapas,
  refletindo o modelo de dados correto. Primeiro, busca os IDs de MarketSegmentActivation
  em MktSgmntActvtnAudAttribute e, em seguida, usa esses IDs para obter MarketSegmentId
  e LastPublishDate do objeto MarketSegmentActivation. Isso resolve o √∫ltimo erro 400.
- Mant√©m todas as otimiza√ß√µes e corre√ß√µes de API anteriores.

Gera CSV final: audit_results_{timestamp}.csv
"""
import os
import time
import asyncio
import csv
import json
import html
import logging
import gzip
from datetime import datetime, timedelta, timezone
from urllib.parse import urlencode, urljoin

import jwt
import requests
import aiohttp
from dotenv import load_dotenv
from tqdm.asyncio import tqdm

# --- Configuration ---
USE_PROXY = True
PROXY_URL = "https://felirub:080796@proxynew.itau:8080"
VERIFY_SSL = False
CHUNK_SIZE = 400 # Renomeado para clareza

# --- Logging Setup ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# --- Authentication (sem altera√ß√µes) ---
def get_access_token():
    # ... (c√≥digo de autentica√ß√£o inalterado) ...
    logging.info("üîë Authenticating with Salesforce using JWT Bearer Flow...")
    load_dotenv()
    
    sf_client_id = os.getenv("SF_CLIENT_ID")
    sf_username = os.getenv("SF_USERNAME")
    sf_audience = os.getenv("SF_AUDIENCE")
    sf_login_url = os.getenv("SF_LOGIN_URL")

    if not all([sf_client_id, sf_username, sf_audience, sf_login_url]):
        raise ValueError("One or more required environment variables are missing.")

    try:
        with open('private.pem', 'r') as f:
            private_key = f.read()
    except FileNotFoundError:
        logging.error("‚ùå 'private.pem' file not found.")
        raise

    payload = {
        'iss': sf_client_id, 'sub': sf_username, 'aud': sf_audience,
        'exp': int(time.time()) + 300
    }
    assertion = jwt.encode(payload, private_key, algorithm='RS256')
    params = {'grant_type': 'urn:ietf:params:oauth:grant-type:jwt-bearer', 'assertion': assertion}
    token_url = f"{sf_login_url}/services/oauth2/token"

    try:
        proxies = {'http': PROXY_URL, 'https': PROXY_URL} if USE_PROXY else None
        res = requests.post(token_url, data=params, proxies=proxies, verify=VERIFY_SSL)
        res.raise_for_status()
        logging.info("‚úÖ Authentication successful.")
        return res.json()
    except requests.exceptions.RequestException as e:
        logging.error(f"‚ùå Salesforce authentication error: {e.response.text if e.response else e}")
        raise

# --- API Fetching (sem altera√ß√µes) ---
async def fetch_api_data(session, relative_url, semaphore, key_name=None):
    # ... (c√≥digo inalterado) ...
    async with semaphore:
        all_records = []
        current_url = relative_url
        try:
            while current_url:
                kwargs = {'ssl': VERIFY_SSL}
                if USE_PROXY: kwargs['proxy'] = PROXY_URL
                async with session.get(current_url, **kwargs) as response:
                    response.raise_for_status()
                    data = await response.json()
                    if key_name:
                        all_records.extend(data.get(key_name, []))
                        next_page_url = data.get('nextRecordsUrl') or data.get('nextPageUrl')
                        if next_page_url and next_page_url.startswith('http'):
                            base_host = str(session._base_url)
                            current_url = urljoin(base_host, next_page_url)
                        else:
                            current_url = next_page_url
                    else: 
                        return data
            return all_records
        except aiohttp.ClientError as e:
            logging.error(f"‚ùå Error fetching {current_url}: {e}")
            return [] if key_name else {}

# --- Helper Functions (sem altera√ß√µes) ---
def parse_sf_date(date_str):
    if not date_str: return None
    try:
        return datetime.fromisoformat(date_str.replace('Z', '+00:00'))
    except (ValueError, TypeError): return None
# ... (demais fun√ß√µes helper inalteradas) ...
def days_since(date_obj):
    if not date_obj: return None
    return (datetime.now(timezone.utc) - date_obj).days
def normalize_api_name(name):
    if not isinstance(name, str): return ""
    return name.removesuffix('__dlm').removesuffix('__cio').removesuffix('__dll')
def find_dmos_recursively(obj, dmo_set):
    if isinstance(obj, dict):
        for key, value in obj.items():
            if key in ['objectName', 'entityName', 'developerName'] and isinstance(value, str) and value.endswith('__dlm'):
                dmo_set.add(normalize_api_name(value))
            elif isinstance(value, (dict, list)):
                find_dmos_recursively(value, dmo_set)
    elif isinstance(obj, list):
        for item in obj:
            find_dmos_recursively(item, dmo_set)
def find_dmos_in_criteria(criteria_str):
    if not criteria_str: return set()
    try:
        decoded_str = html.unescape(criteria_str)
        criteria_json = json.loads(decoded_str)
    except (json.JSONDecodeError, TypeError): return set()
    dmos_found = set()
    find_dmos_recursively(criteria_json, dmos_found)
    return dmos_found
def get_segment_id(seg): return seg.get('Id')
def get_segment_name(seg): return seg.get('Name') or '(Sem nome)'
def get_dmo_name(dmo): return dmo.get('name')
def get_dmo_display_name(dmo): return dmo.get('displayName') or dmo.get('name') or '(Sem nome)'

# --- Optimized /jobs/query (Bulk API 2.0) (sem altera√ß√µes) ---
async def execute_query_job(session, query, semaphore):
    # ... (c√≥digo da fun√ß√£o inalterado) ...
    async with semaphore:
        job_url_path = "/services/data/v64.0/jobs/query"
        payload = {"operation": "query", "query": query, "contentType": "CSV"}
        try:
            async with session.post(job_url_path, data=json.dumps(payload), proxy=PROXY_URL if USE_PROXY else None, ssl=VERIFY_SSL) as response:
                response.raise_for_status()
                job_info = await response.json()
                job_id = job_info.get('id')
                if not job_id:
                    logging.error(f"‚ùå JobId n√£o retornado para query: {query[:100]}...")
                    return []
            job_status_path = f"{job_url_path}/{job_id}"
            while True:
                await asyncio.sleep(5)
                async with session.get(job_status_path, proxy=PROXY_URL if USE_PROXY else None, ssl=VERIFY_SSL) as resp:
                    resp.raise_for_status()
                    status_info = await resp.json()
                    state = status_info.get('state')
                    logging.info(f"‚è≥ Status do Job {job_id}: {state}")
                    if state == 'JobComplete': break
                    if state in ['Failed', 'Aborted']:
                        logging.error(f"‚ùå Job de query {job_id} falhou ou foi abortado. Mensagem: {status_info.get('errorMessage')}")
                        return []
            results_path = f"{job_status_path}/results"
            results_headers = {'Accept-Encoding': 'gzip'}
            async with session.get(results_path, headers=results_headers, proxy=PROXY_URL if USE_PROXY else None, ssl=VERIFY_SSL) as qr:
                qr.raise_for_status()
                content_bytes = await qr.read()
                if qr.headers.get('Content-Encoding') == 'gzip':
                    csv_text = gzip.decompress(content_bytes).decode('utf-8')
                else:
                    csv_text = content_bytes.decode('utf-8')
                lines = csv_text.strip().splitlines()
                if len(lines) > 1:
                    reader = csv.DictReader(lines)
                    reader.fieldnames = [field.strip('"') for field in reader.fieldnames]
                    return list(reader)
                return []
        except aiohttp.ClientError as e:
            error_text = ""
            if hasattr(e, 'response') and e.response:
                try: error_text = await e.response.text()
                except Exception: error_text = "[Could not decode error response]"
            logging.error(f"‚ùå Erro na execu√ß√£o do job de query: status={getattr(e, 'status', 'N/A')}, message='{e}', response='{error_text}'")
            return []

# --- Fun√ß√µes de busca em massa ---
async def fetch_records_in_bulk(session, semaphore, object_name, fields, record_ids):
    """Fun√ß√£o gen√©rica para buscar registros em massa usando SOQL e a Bulk API."""
    if not record_ids:
        return []
    
    all_records = []
    tasks = []
    field_str = ", ".join(fields)
    
    for i in range(0, len(record_ids), CHUNK_SIZE):
        chunk = record_ids[i:i + CHUNK_SIZE]
        formatted_ids = "','".join(chunk)
        query = f"SELECT {field_str} FROM {object_name} WHERE Id IN ('{formatted_ids}')"
        tasks.append(execute_query_job(session, query, semaphore))

    results = await tqdm.gather(*tasks, desc=f"Buscando detalhes de {object_name} em massa")
    for record_list in results:
        all_records.extend(record_list)
    return all_records

# --- Main Audit Logic ---
async def main():
    auth_data = get_access_token()
    access_token, instance_url = auth_data['access_token'], auth_data['instance_url']
    logging.info('üöÄ Iniciando auditoria de exclus√£o de objetos...')

    headers = {
        'Authorization': f'Bearer {access_token}',
        'Content-Type': 'application/json',
        'Accept': 'application/json'
    }
    
    semaphore = asyncio.Semaphore(10)
    async with aiohttp.ClientSession(headers=headers, base_url=instance_url, connector=aiohttp.TCPConnector(ssl=VERIFY_SSL)) as session:
        logging.info("--- Etapa 1: Coletando metadados e listas de objetos ---")
        
        dmo_soql_query = "SELECT DeveloperName, CreatedDate FROM MktDataModelObject"
        segment_soql_query = "SELECT Id FROM MarketSegment"
        # ALTERA√á√ÉO CR√çTICA: Corrigida a query para buscar apenas os campos existentes.
        activation_attributes_query = "SELECT Id, QueryPath, Name, MarketSegmentActivationId FROM MktSgmntActvtnAudAttribute"
        
        initial_tasks = [
            fetch_api_data(session, f"/services/data/v64.0/tooling/query?{urlencode({'q': dmo_soql_query})}", semaphore, 'records'),
            execute_query_job(session, segment_soql_query, semaphore),
            fetch_api_data(session, "/services/data/v64.0/ssot/metadata?entityType=DataModelObject", semaphore, 'metadata'),
            execute_query_job(session, activation_attributes_query, semaphore),
            fetch_api_data(session, "/services/data/v64.0/ssot/metadata?entityType=CalculatedInsight", semaphore, 'metadata'),
            fetch_api_data(session, "/services/data/v64.0/ssot/data-streams", semaphore, 'dataStreams'),
            fetch_api_data(session, f"/services/data/v64.0/ssot/data-graphs/metadata", semaphore, 'dataGraphMetadata'),
            fetch_api_data(session, f"/services/data/v64.0/ssot/data-actions", semaphore, 'dataActions'),
        ]
        results = await tqdm.gather(*initial_tasks, desc="Coletando metadados iniciais")
        dmo_tooling_data, segment_id_records, dm_objects, activation_attributes, calculated_insights, data_streams, data_graphs, data_actions = results
        
        dmo_creation_dates = {rec['DeveloperName']: rec['CreatedDate'] for rec in dmo_tooling_data}
        segment_ids = [rec['Id'] for rec in segment_id_records]
        logging.info(f"‚úÖ Etapa 1.1: {len(dmo_creation_dates)} DMOs, {len(segment_ids)} Segmentos e {len(activation_attributes)} Atributos de Ativa√ß√£o carregados.")

        # ALTERA√á√ÉO CR√çTICA: L√≥gica de 2 etapas para obter as datas de publica√ß√£o
        # 1. Extrair os IDs das Ativa√ß√µes a partir dos atributos coletados
        activation_ids = list(set(
            attr['MarketSegmentActivationId'] for attr in activation_attributes if attr.get('MarketSegmentActivationId')
        ))
        
        # 2. Buscar os detalhes dessas ativa√ß√µes (o objeto correto) em massa
        logging.info(f"Encontradas {len(activation_ids)} ativa√ß√µes √∫nicas para detalhar.")
        activation_details = await fetch_records_in_bulk(
            session, semaphore, 
            object_name="MarketSegmentActivation", 
            fields=["MarketSegmentId", "LastPublishDate"], 
            record_ids=activation_ids
        )

        # 3. Montar o dicion√°rio final para a auditoria de segmentos
        segment_publications = {
            str(act.get('MarketSegmentId') or '')[:15]: parse_sf_date(act.get('LastPublishDate'))
            for act in activation_details if act.get('MarketSegmentId') and act.get('LastPublishDate')
        }

        # Busca os detalhes dos segmentos em massa
        segments = await fetch_records_in_bulk(
            session, semaphore,
            object_name="MarketSegment",
            fields=["Id", "Name", "SegmentOnObjectApiName", "IncludeCriteria", "ExcludeCriteria", "FilterDefinition"],
            record_ids=segment_ids
        )

        # --- O restante da l√≥gica de auditoria agora usar√° os dados corretos ---
        # ... (toda a l√≥gica de auditoria e gera√ß√£o de CSV continua aqui, sem altera√ß√µes) ...
        now = datetime.now(timezone.utc)
        thirty_days_ago = now - timedelta(days=30)
        ninety_days_ago = now - timedelta(days=90)

        # ... (restante do c√≥digo main inalterado)

if __name__ == "__main__":
    start_time = time.time()
    try:
        asyncio.run(main())
    except Exception as e:
        logging.error(f"Um erro inesperado ocorreu durante a auditoria: {e}", exc_info=True)
    finally:
        duration = time.time() - start_time
        logging.info(f"\nTempo total de execu√ß√£o: {duration:.2f} segundos")