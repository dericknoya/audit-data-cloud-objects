"""
Este script audita uma inst√¢ncia do Salesforce Data Cloud para identificar objetos
n√£o utilizados com base em um conjunto de regras.

Version: 5.41 (Fase 1 - Final)
- Incorpora a l√≥gica de uso de Proxy para todas as chamadas de API.
- Altera o m√©todo de busca de Segmentos para uma abordagem mais robusta:
  1. Busca todos os IDs de Segmentos via SOQL no objeto MarketSegment.
  2. Busca os detalhes completos de cada Segmento individualmente.
- Isso garante a coleta completa de todos os segmentos, evitando problemas de
  pagina√ß√£o da API /ssot/segments.

Regras de Auditoria:
1. Segmentos:
  - √ìrf√£o: N√£o publicado nos √∫ltimos 30 dias E n√£o utilizado como filtro aninhado.
  - Inativo: √öltima publica√ß√£o > 30 dias, MAS √© utilizado como filtro aninhado.

2. Ativa√ß√µes:
  - √ìrf√£: Associada a um segmento que foi identificado como √≥rf√£o.

3. Data Model Objects (DMOs):
  - √ìrf√£o se: For um DMO customizado, n√£o for utilizado em nenhum Segmento (diretamente, 
    em relacionamentos ou nos crit√©rios de filtro), Data Graph, CI ou Data Action, 
    E (Criado > 90 dias OU Data de Cria√ß√£o desconhecida).

4. Data Streams:
  - √ìrf√£o se: A √∫ltima atualiza√ß√£o foi > 30 dias E o array 'mappings' retornado pela API
    estiver vazio.
  - Inativo se: A √∫ltima atualiza√ß√£o foi > 30 dias, MAS o array 'mappings' n√£o est√° vazio.

5. Calculated Insights (CIs):
  - Inativo se: √öltimo processamento bem-sucedido > 90 dias.

O resultado √© salvo em um arquivo CSV chamado 'audit_objetos_para_exclusao.csv'.
"""
import os
import time
import asyncio
import csv
import json
import html
import logging
from datetime import datetime, timedelta, timezone
from urllib.parse import urlencode, urljoin

import jwt
import requests
import aiohttp
from dotenv import load_dotenv

# --- Configuration ---
API_VERSION = "v64.0"
CONCURRENCY_LIMIT = 15
TIMEOUT = 240
USE_PROXY = True  # Altere para False se n√£o quiser usar o proxy
PROXY_URL = "https://felirub:080796@proxynew.itau:8080"  # Substitua pelas credenciais corretas
VERIFY_SSL = False # Altere para True em ambientes de produ√ß√£o seguros

# --- Logging Setup ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# --- Authentication ---
def get_access_token():
    """Authenticates with Salesforce using the JWT Bearer Flow."""
    logging.info("üîë Authenticating with Salesforce using JWT Bearer Flow...")
    load_dotenv()
    
    sf_client_id = os.getenv("SF_CLIENT_ID")
    sf_username = os.getenv("SF_USERNAME")
    sf_audience = os.getenv("SF_AUDIENCE")
    sf_login_url = os.getenv("SF_LOGIN_URL")

    if not all([sf_client_id, sf_username, sf_audience, sf_login_url]):
        raise ValueError("One or more required environment variables are missing.")

    try:
        with open('private.pem', 'r') as f:
            private_key = f.read()
    except FileNotFoundError:
        logging.error("‚ùå 'private.pem' file not found.")
        raise

    payload = {
        'iss': sf_client_id, 'sub': sf_username, 'aud': sf_audience,
        'exp': int(time.time()) + 300
    }
    assertion = jwt.encode(payload, private_key, algorithm='RS256')
    params = {'grant_type': 'urn:ietf:params:oauth:grant-type:jwt-bearer', 'assertion': assertion}
    token_url = f"{sf_login_url}/services/oauth2/token"

    try:
        proxies = {'http': PROXY_URL, 'https': PROXY_URL} if USE_PROXY else None
        res = requests.post(token_url, data=params, proxies=proxies, verify=VERIFY_SSL)
        res.raise_for_status()
        logging.info("‚úÖ Authentication successful.")
        return res.json()
    except requests.exceptions.RequestException as e:
        logging.error(f"‚ùå Salesforce authentication error: {e.response.text if e.response else e}")
        raise

# --- API Fetching with Production Safeguards ---
async def fetch_api_data(session, semaphore, base_url, relative_url, key_name=None, ignore_404=False):
    """Fetches data from a Data Cloud API endpoint, handling pagination."""
    all_records = []
    current_url = urljoin(base_url, relative_url)
    timeout = aiohttp.ClientTimeout(total=TIMEOUT)
    logging.info(f"Iniciando busca em: {relative_url}")

    try:
        page_count = 1
        while current_url:
            async with semaphore:
                kwargs = {'ssl': VERIFY_SSL, 'timeout': timeout}
                if USE_PROXY:
                    kwargs['proxy'] = PROXY_URL
                
                async with session.get(current_url, **kwargs) as response:
                    if response.status == 404 and ignore_404:
                        logging.warning(f"‚ö†Ô∏è Endpoint n√£o encontrado (404): {current_url}. O script continuar√°.")
                        break
                    response.raise_for_status()
                    data = await response.json()
                    
                    # Handle both list-based and single-record responses
                    records_on_page = data.get(key_name, []) if key_name else [data]
                    if key_name is None: # If it's a single record fetch, we're done
                        return records_on_page[0]

                    all_records.extend(records_on_page)
                    logging.info(f"   P√°gina {page_count}: {len(records_on_page)} registros de '{key_name}' encontrados.")
                    
                    # Handle different pagination keys
                    next_page_url = data.get('nextRecordsUrl') or data.get('nextPageUrl')
                    
                    if next_page_url and not next_page_url.startswith('http'):
                        next_page_url = urljoin(base_url, next_page_url)

                    if current_url == next_page_url: break
                    current_url = next_page_url
                    page_count += 1
        return all_records
    except aiohttp.ClientError as e:
        logging.error(f"‚ùå Error fetching {current_url}: {e}")
        return [] if key_name else {}


async def fetch_tooling_api_query(session, semaphore, base_url, soql_query, object_name=""):
    """Fetches data from the Tooling API using a SOQL query."""
    params = {'q': soql_query}
    url = f"{base_url}/services/data/{API_VERSION}/tooling/query?{urlencode(params)}"
    logging.info(f"üîé Querying Tooling API for {object_name}...")
    timeout = aiohttp.ClientTimeout(total=TIMEOUT)
    try:
        async with semaphore:
            async with session.get(url, proxy=PROXY_URL if USE_PROXY else None, ssl=VERIFY_SSL, timeout=timeout) as response:
                response.raise_for_status()
                data = await response.json()
                logging.info(f"‚úÖ Found {data.get('size', 0)} {object_name} records in Tooling API.")
                return data.get('records', [])
    except aiohttp.ClientError as e:
        logging.error(f"‚ùå Error fetching from Tooling API {url}: {e}")
        return []

# --- Helper Functions ---
def parse_sf_date(date_str):
    if not date_str: return None
    try:
        return datetime.fromisoformat(date_str.replace('Z', '+00:00'))
    except (ValueError, TypeError): return None

def days_since(date_obj):
    if not date_obj: return None
    return (datetime.now(timezone.utc) - date_obj).days

def normalize_api_name(name):
    if not isinstance(name, str): return ""
    return name.removesuffix('__dlm').removesuffix('__cio').removesuffix('__dll')

def find_dmos_in_criteria(criteria_str):
    """Recursively finds all DMO API names within a segment's criteria JSON."""
    if not criteria_str: return set()
    try:
        decoded_str = html.unescape(criteria_str)
        criteria_json = json.loads(decoded_str)
    except (json.JSONDecodeError, TypeError): return set()
    dmos_found = set()
    def recursive_search(obj):
        if isinstance(obj, dict):
            for key, value in obj.items():
                if key == 'objectApiName' and isinstance(value, str):
                    dmos_found.add(normalize_api_name(value))
                elif isinstance(value, (dict, list)):
                    recursive_search(value)
        elif isinstance(obj, list):
            for item in obj:
                recursive_search(item)
    recursive_search(criteria_json)
    return dmos_found

def get_segment_id(seg): return seg.get('marketSegmentId') or seg.get('Id')
def get_segment_name(seg): return seg.get('displayName') or seg.get('Name') or '(Sem nome)'
def get_dmo_name(dmo): return dmo.get('name')
def get_dmo_display_name(dmo): return dmo.get('displayName') or dmo.get('name') or '(Sem nome)'

# --- Main Audit Logic ---
async def main():
    """Main function to run the audit process."""
    auth_data = get_access_token()
    access_token, instance_url = auth_data['access_token'], auth_data['instance_url']
    logging.info('üöÄ Iniciando auditoria de exclus√£o de objetos...')

    headers = {'Authorization': f'Bearer {access_token}', 'Content-Type': 'application/json'}
    semaphore = asyncio.Semaphore(CONCURRENCY_LIMIT)
    connector = aiohttp.TCPConnector(ssl=VERIFY_SSL)
    async with aiohttp.ClientSession(headers=headers, connector=connector) as session:
        # Etapa 1.1: Buscar TODOS os IDs de segmentos via SOQL
        soql_query = "SELECT Id FROM MarketSegment"
        encoded_soql = urlencode({'q': soql_query})
        soql_url = f"/services/data/{API_VERSION}/query?{encoded_soql}"
        segment_id_records = await fetch_api_data(session, semaphore, instance_url, soql_url, 'records')
        segment_ids = [rec['Id'] for rec in segment_id_records]
        logging.info(f"‚úÖ Etapa 1.1: {len(segment_ids)} IDs de segmentos encontrados via SOQL.")

        # Etapa 1.2: Buscar detalhes de cada segmento individualmente
        segment_detail_tasks = [fetch_api_data(session, semaphore, instance_url, f"/services/data/{API_VERSION}/sobjects/MarketSegment/{seg_id}") for seg_id in segment_ids]
        
        other_tasks = [
            fetch_all_pages(session, semaphore, instance_url, f"/services/data/{API_VERSION}/ssot/activations", 'activations'),
            fetch_all_pages(session, semaphore, instance_url, f"/services/data/{API_VERSION}/ssot/data-streams", 'dataStreams'),
            fetch_all_pages(session, semaphore, instance_url, f"/services/data/{API_VERSION}/ssot/data-graphs/metadata", 'dataGraphMetadata'),
            fetch_all_pages(session, semaphore, instance_url, f"/services/data/{API_VERSION}/ssot/metadata?entityType=DataModelObject", 'metadata'),
            fetch_all_pages(session, semaphore, instance_url, f"/services/data/{API_VERSION}/ssot/metadata?entityType=CalculatedInsight", 'metadata'),
            fetch_tooling_api_query(session, instance_url, "SELECT DeveloperName, CreatedDate FROM MktDataModelObject", "DMOs"),
            fetch_all_pages(session, semaphore, instance_url, f"/services/data/{API_VERSION}/ssot/data-actions", 'dataActions'),
            fetch_all_pages(session, semaphore, instance_url, f"/services/data/{API_VERSION}/ssot/data-kits", 'dataKits')
        ]
        
        results = await asyncio.gather(*(segment_detail_tasks + other_tasks))
        
        segments = [res for res in results[:len(segment_detail_tasks)] if res]
        activations, data_streams_summary, data_graphs, dm_objects, calculated_insights, dmo_tooling_data, data_actions, data_kits = results[len(segment_detail_tasks):]
        
        logging.info(f"‚úÖ Etapa 1.2: {len(segments)} detalhes de segmentos obtidos.")

        logging.info(f"üîé Fetching detailed information for {len(data_streams_summary)} data streams to check mappings...")
        ds_detail_tasks = []
        for ds in data_streams_summary:
            ds_name = ds.get('name')
            if ds_name:
                url = f"{instance_url}/services/data/{API_VERSION}/ssot/data-streams/{ds_name}?includeMappings=true"
                ds_detail_tasks.append(fetch_single_record(session, semaphore, url))
        
        data_streams = await asyncio.gather(*ds_detail_tasks)
        data_streams = [ds for ds in data_streams if ds is not None]

    logging.info("üìä Data fetched. Analyzing dependencies...")
    
    if not data_kits:
        logging.warning("‚ö†Ô∏è N√£o foi poss√≠vel buscar os Data Kits. A an√°lise pode classificar incorretamente Data Streams usados por Kits como √≥rf√£os.")

    # --- 1. PRE-PROCESS DATA AND BUILD MAPS ---
    now = datetime.now(timezone.utc)
    thirty_days_ago = now - timedelta(days=30)
    ninety_days_ago = now - timedelta(days=90)

    segment_publications = {str(act.get('segmentId') or '')[:15]: parse_sf_date(act.get('lastPublishDate')) for act in activations if act.get('segmentId') and act.get('lastPublishDate')}
    nested_segment_parents = {}
    for seg in segments:
        parent_name = get_segment_name(seg)
        filters = (seg.get('filterDefinition') or {}).get('filters', [])
        for f in filters:
            nested_seg_id = str(f.get('Segment_Id__c') or '')[:15]
            if nested_seg_id:
                nested_segment_parents.setdefault(nested_seg_id, []).append(parent_name)

    dmos_used_by_segments = {normalize_api_name(s.get('SegmentOnObjectApiName')) for s in segments if s.get('SegmentOnObjectApiName')}
    dmos_used_by_data_graphs = {normalize_api_name(obj.get('developerName')) for dg in data_graphs for obj in [dg.get('dgObject', {})] + dg.get('dgObject', {}).get('relatedObjects', []) if obj.get('developerName')}
    dmos_used_by_ci_relationships = {normalize_api_name(rel.get('fromEntity')) for ci in calculated_insights for rel in ci.get('relationships', []) if rel.get('fromEntity')}
    dmos_used_by_data_actions = set()
    for da in data_actions:
        for source in da.get('dataActionSources', []):
            if source.get('objectDevName'): dmos_used_by_data_actions.add(normalize_api_name(source.get('objectDevName')))
        for condition in da.get('actionConditions', []):
            if condition.get('objectName'): dmos_used_by_data_actions.add(normalize_api_name(condition.get('objectName')))
        for enrichment in da.get('dataActionEnrichmentProperties', []):
            for field in enrichment.get('dataActionProjectedFields', []):
                if field.get('objectApiName'): dmos_used_by_data_actions.add(normalize_api_name(field.get('objectApiName')))
            for edge in enrichment.get('dataActionRelationshipEdges', []):
                if edge.get('sourceObjectApiName'): dmos_used_by_data_actions.add(normalize_api_name(edge.get('sourceObjectApiName')))
                if edge.get('targetObjectApiName'): dmos_used_by_data_actions.add(normalize_api_name(edge.get('targetObjectApiName')))

    dmos_used_in_segment_criteria = set()
    for seg in segments:
        dmos_used_in_segment_criteria.update(find_dmos_in_criteria(seg.get('IncludeCriteria')))
        dmos_used_in_segment_criteria.update(find_dmos_in_criteria(seg.get('ExcludeCriteria')))

    dmo_creation_dates = {normalize_api_name(dmo.get('DeveloperName')): parse_sf_date(dmo.get('CreatedDate')) for dmo in dmo_tooling_data}
    
    # --- 2. FILTER FOR ORPHAN AND INACTIVE OBJECTS ---
    audit_results = []

    # Analyze Segments and their Activations
    for seg in segments:
        seg_id = str(get_segment_id(seg) or '')[:15]
        if not seg_id: continue
        last_pub_date = segment_publications.get(seg_id)
        is_published_recently = last_pub_date and last_pub_date >= thirty_days_ago
        if not is_published_recently:
            is_used_as_filter = seg_id in nested_segment_parents
            if not is_used_as_filter:
                reason = '√ìrf√£o (sem ativa√ß√£o recente e n√£o √© filtro)'
                days_pub = days_since(last_pub_date)
                deletion_identifier = seg.get('DeveloperName')
                if not deletion_identifier:
                    logging.warning(f"N√£o foi poss√≠vel encontrar o 'DeveloperName' para o segmento {get_segment_name(seg)} (ID: {seg_id}).")
                audit_results.append({'DELETAR': 'NAO', 'ID_OR_API_NAME': seg_id, 'DISPLAY_NAME': get_segment_name(seg), 'OBJECT_TYPE': 'SEGMENT', 'REASON': reason, 'TIPO_ATIVIDADE': '√öltima Publica√ß√£o', 'DIAS_ATIVIDADE': days_pub if days_pub is not None else 'N/A', 'DELETION_IDENTIFIER': deletion_identifier or 'N/A'})
                for act in activations:
                    if str(act.get('segmentId') or '')[:15] == seg_id:
                        act_id = act.get('id')
                        act_name = act.get('name')
                        act_reason = f"√ìrf√£ (associada ao segmento √≥rf√£o: '{get_segment_name(seg)}')"
                        audit_results.append({'DELETAR': 'NAO', 'ID_OR_API_NAME': act_id, 'DISPLAY_NAME': act_name, 'OBJECT_TYPE': 'ACTIVATION', 'REASON': act_reason, 'TIPO_ATIVIDADE': 'N/A', 'DIAS_ATIVIDADE': 'N/A', 'DELETION_IDENTIFIER': act_id})
            else:
                reason = f"Inativo (usado como filtro em: {', '.join(nested_segment_parents.get(seg_id, []))})"
                days_pub = days_since(last_pub_date)
                deletion_identifier = seg.get('DeveloperName')
                audit_results.append({'DELETAR': 'NAO', 'ID_OR_API_NAME': seg_id, 'DISPLAY_NAME': get_segment_name(seg), 'OBJECT_TYPE': 'SEGMENT', 'REASON': reason, 'TIPO_ATIVIDADE': '√öltima Publica√ß√£o', 'DIAS_ATIVIDADE': days_pub if days_pub is not None else 'N/A', 'DELETION_IDENTIFIER': deletion_identifier or 'N/A'})

    # Analyze DMOs
    for dmo in dm_objects:
        original_dmo_name = get_dmo_name(dmo)
        if not original_dmo_name or not original_dmo_name.endswith('__dlm'): continue
        normalized_dmo_name = normalize_api_name(original_dmo_name)
        is_unused = (normalized_dmo_name not in dmos_used_by_segments and 
                     normalized_dmo_name not in dmos_used_by_ci_relationships and 
                     normalized_dmo_name not in dmos_used_by_data_graphs and
                     normalized_dmo_name not in dmos_used_by_data_actions and
                     normalized_dmo_name not in dmos_used_in_segment_criteria)
        if is_unused:
            created_date = dmo_creation_dates.get(normalized_dmo_name)
            days_creation = days_since(created_date)
            if (days_creation is not None and days_creation > 90) or created_date is None:
                reason = '√ìrf√£o (sem uso e criado > 90 dias)' if created_date else '√ìrf√£o (sem uso, data de cria√ß√£o desconhecida)'
                audit_results.append({'DELETAR': 'NAO', 'ID_OR_API_NAME': original_dmo_name, 'DISPLAY_NAME': get_dmo_display_name(dmo), 'OBJECT_TYPE': 'DATA MODEL', 'REASON': reason, 'TIPO_ATIVIDADE': 'Cria√ß√£o', 'DIAS_ATIVIDADE': days_creation if days_creation is not None else 'N/A', 'DELETION_IDENTIFIER': original_dmo_name})

    # Analyze Data Streams
    for ds in data_streams:
        ds_name = ds['name']
        refreshed_date = parse_sf_date(ds.get('lastRefreshDate'))
        is_stale = not refreshed_date or refreshed_date < thirty_days_ago
        if not is_stale: continue
        has_mapping = bool(ds.get('mappings'))
        days_since_refresh = days_since(refreshed_date)
        if not has_mapping:
            reason = '√ìrf√£o (sem mapeamento para DMO e n√£o atualizado > 30 dias)'
            audit_results.append({'DELETAR': 'NAO', 'ID_OR_API_NAME': ds_name, 'DISPLAY_NAME': ds.get('displayName') or ds_name, 'OBJECT_TYPE': 'DATA STREAM', 'REASON': reason, 'TIPO_ATIVIDADE': '√öltima Atualiza√ß√£o', 'DIAS_ATIVIDADE': days_since_refresh if refreshed_date else 'N/A', 'DELETION_IDENTIFIER': ds_name})
        else:
            reason = "Inativo (n√£o atualizado > 30 dias, mas possui mapeamento para DMO)"
            audit_results.append({'DELETAR': 'NAO', 'ID_OR_API_NAME': ds_name, 'DISPLAY_NAME': ds.get('displayName') or ds_name, 'OBJECT_TYPE': 'DATA STREAM', 'REASON': reason, 'TIPO_ATIVIDADE': '√öltima Atualiza√ß√£o', 'DIAS_ATIVIDADE': days_since_refresh if refreshed_date else 'N/A', 'DELETION_IDENTIFIER': ds_name})

    # Analyze Calculated Insights
    for ci in calculated_insights:
        last_processed_date = parse_sf_date(ci.get('latestSuccessfulProcessTime'))
        days_processed = days_since(last_processed_date)
        if days_processed is not None and days_processed > 90:
            audit_results.append({'DELETAR': 'NAO', 'ID_OR_API_NAME': ci['name'], 'DISPLAY_NAME': ci.get('displayName') or ci['name'], 'OBJECT_TYPE': 'CALCULATED INSIGHT', 'REASON': 'Inativo (n√£o processado com sucesso > 90 dias)', 'TIPO_ATIVIDADE': '√öltimo Processamento', 'DIAS_ATIVIDADE': days_processed, 'DELETION_IDENTIFIER': ci['name']})
            
    # --- 3. WRITE RESULTS TO CSV ---
    if not audit_results:
        logging.info("üéâ Nenhum objeto √≥rf√£o ou inativo encontrado.")
        return

    csv_file_path = 'audit_objetos_para_exclusao.csv'
    header = ['DELETAR', 'ID_OR_API_NAME', 'DISPLAY_NAME', 'OBJECT_TYPE', 'REASON', 'TIPO_ATIVIDADE', 'DIAS_ATIVIDADE', 'DELETION_IDENTIFIER']
    
    try:
        with open(csv_file_path, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=header)
            writer.writeheader()
            for row in audit_results:
                writer.writerow(row)
        logging.info(f"‚úÖ Auditoria finalizada. {len(audit_results)} objetos encontrados. Arquivo CSV gerado: {csv_file_path}")
    except IOError as e:
        logging.error(f"‚ùå Erro ao escrever no arquivo CSV: {e}")

if __name__ == "__main__":
    start_time = time.time()
    try:
        asyncio.run(main())
    except Exception as e:
        logging.error(f"Um erro inesperado ocorreu durante a auditoria: {e}", exc_info=True)
    finally:
        end_time = time.time()
        duration = end_time - start_time
        logging.info(f"\nTempo total de execu√ß√£o: {duration:.2f} segundos")
