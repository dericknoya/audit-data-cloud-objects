"""
Este script audita uma inst√¢ncia do Salesforce Data Cloud para identificar 
campos de DMOs (Data Model Objects) n√£o utilizados.

Metodologia:
- Utiliza o fluxo de autentica√ß√£o JWT Bearer Flow (com certificado).
- Um campo "n√£o utilizado" √© aquele que n√£o √© encontrado em Segmentos, Ativa√ß√µes ou CIs.
- Audita o uso de campos analisando os crit√©rios de Segmentos ('includeCriteria'/'excludeCriteria').
- Audita o uso de campos buscando os metadados detalhados de cada Ativa√ß√£o.
- Audita o uso de campos e DMOs dentro de Calculated Insights.
- O relat√≥rio final inclui os Nomes de Exibi√ß√£o do DMO e do Campo para melhor legibilidade.
- Exclui campos e DMOs de sistema/gerados automaticamente da an√°lise.
- Adiciona uma coluna 'DELETAR' como a primeira coluna, com o valor padr√£o 'NAO'.
"""
import os
import time
import asyncio
import csv
import json
import html
import logging
from collections import defaultdict
from urllib.parse import urljoin

import jwt
import requests
import aiohttp
from dotenv import load_dotenv

# --- Configura√ß√£o de Rede ---
USE_PROXY = True
PROXY_URL = "http://seu_usuario:sua_senha@host:porta" # Substitua pelo seu proxy
VERIFY_SSL = False

# --- Configura√ß√£o do Logging ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# --- Autentica√ß√£o ---
def get_access_token():
    """Autentica com o Salesforce usando o fluxo JWT Bearer Flow."""
    logging.info("üîë Autenticando com o Salesforce via JWT (certificado)...")
    load_dotenv()
    
    sf_client_id = os.getenv("SF_CLIENT_ID")
    sf_username = os.getenv("SF_USERNAME")
    sf_audience = os.getenv("SF_AUDIENCE")
    sf_login_url = os.getenv("SF_LOGIN_URL")

    if not all([sf_client_id, sf_username, sf_audience, sf_login_url]):
        raise ValueError("Uma ou mais vari√°veis de ambiente para o fluxo JWT est√£o faltando.")
    
    try:
        with open('private.pem', 'r') as f: 
            private_key = f.read()
    except FileNotFoundError:
        logging.error("‚ùå Erro: Arquivo 'private.pem' n√£o encontrado."); raise
        
    payload = {'iss': sf_client_id, 'sub': sf_username, 'aud': sf_audience, 'exp': int(time.time()) + 300}
    assertion = jwt.encode(payload, private_key, algorithm='RS256')
    params = {'grant_type': 'urn:ietf:params:oauth:grant-type:jwt-bearer', 'assertion': assertion}
    token_url = f"{sf_login_url}/services/oauth2/token"
    
    try:
        proxies = {'http': PROXY_URL, 'https': PROXY_URL} if USE_PROXY else None
        res = requests.post(token_url, data=params, proxies=proxies, verify=VERIFY_SSL)
        res.raise_for_status()
        logging.info("‚úÖ Autentica√ß√£o bem-sucedida.")
        return res.json()
    except requests.exceptions.RequestException as e:
        logging.error(f"‚ùå Erro na autentica√ß√£o com Salesforce: {e.response.text if e.response else e}"); raise


# --- API Fetching ---
async def fetch_api_data(session, instance_url, relative_url, key_name=None):
    """Fun√ß√£o gen√©rica ass√≠ncrona para buscar dados, com suporte a proxy e ssl."""
    all_records = []
    current_url = urljoin(instance_url, relative_url)
    try:
        while current_url:
            kwargs = {'ssl': VERIFY_SSL}
            if USE_PROXY:
                kwargs['proxy'] = PROXY_URL

            async with session.get(current_url, **kwargs) as response:
                response.raise_for_status(); data = await response.json()
                if key_name:
                    all_records.extend(data.get(key_name, []))
                    next_page_url = data.get('nextPageUrl')
                    if next_page_url and not next_page_url.startswith('http'):
                        next_page_url = urljoin(instance_url, next_page_url)
                else: 
                    return data
                if current_url == next_page_url: break
                current_url = next_page_url
        return all_records
    except aiohttp.ClientError as e:
        logging.error(f"‚ùå Erro ao buscar {current_url}: {e}"); return [] if key_name else {}

# --- Helper Functions ---
def _recursive_find_fields(obj, used_fields_set):
    if isinstance(obj, dict):
        for key, value in obj.items():
            if key == 'fieldApiName' and isinstance(value, str):
                used_fields_set.add(value)
            elif key == 'name' and 'type' in obj and isinstance(value, str):
                 used_fields_set.add(value)
            elif isinstance(value, (dict, list)):
                _recursive_find_fields(value, used_fields_set)
    elif isinstance(obj, list):
        for item in obj:
            _recursive_find_fields(item, used_fields_set)

def parse_json_from_string(json_string, used_fields_set):
    """Decodifica e analisa uma string JSON para encontrar campos."""
    if not json_string: return
    try:
        decoded_str = html.unescape(json_string)
        data = json.loads(decoded_str)
        _recursive_find_fields(data, used_fields_set)
    except (json.JSONDecodeError, TypeError):
        logging.warning(f"‚ö†Ô∏è Falha ao processar crit√©rio de segmento: {json_string[:100]}...")


# --- Main Audit Logic ---
async def audit_dmo_fields():
    auth_data = get_access_token()
    access_token = auth_data['access_token']
    instance_url = auth_data['instance_url']
    logging.info('üöÄ Iniciando auditoria de campos de DMO n√£o utilizados...')
    headers = {'Authorization': f'Bearer {access_token}', 'Content-Type': 'application/json'}

    async with aiohttp.ClientSession(headers=headers) as session:
        logging.info("--- Etapa 1: Coletando metadados e listas de objetos ---")
        base_tasks = [
            fetch_api_data(session, instance_url, "/services/data/v64.0/ssot/metadata?entityType=DataModelObject", 'metadata'),
            fetch_api_data(session, instance_url, "/services/data/v64.0/ssot/segments", 'segments'),
            fetch_api_data(session, instance_url, "/services/data/v64.0/ssot/activations", 'activations'),
            fetch_api_data(session, instance_url, "/services/data/v64.0/ssot/metadata?entityType=CalculatedInsight", 'metadata'),
        ]
        dmo_metadata_list, segments_list, activations_summary, calculated_insights = await asyncio.gather(*base_tasks)
        
        logging.info("\n--- Etapa 2: Coletando detalhes das Ativa√ß√µes ---")
        activation_detail_tasks = [fetch_api_data(session, instance_url, f"/services/data/v64.0/ssot/activations/{act.get('id')}") for act in activations_summary if act.get('id')]
        logging.info(f"üîé Buscando detalhes para {len(activation_detail_tasks)} ativa√ß√µes...")
        detailed_activations = await asyncio.gather(*activation_detail_tasks)

    logging.info("\nüìä Dados coletados. Analisando o uso dos campos...")
    
    all_dmo_data = defaultdict(lambda: {'fields': {}, 'displayName': ''})
    dmo_prefixes_to_exclude = ('ssot', 'unified', 'individual', 'einstein', 'segment_membership', 'aa_', 'aal_')

    for dmo in dmo_metadata_list:
        if (dmo_name := dmo.get('name')) and dmo_name.endswith('__dlm'):
            dmo_name_lower = dmo_name.lower()
            if any(dmo_name_lower.startswith(prefix) for prefix in dmo_prefixes_to_exclude):
                continue
            all_dmo_data[dmo_name]['displayName'] = dmo.get('displayName', dmo_name)
            for field in dmo.get('fields', []):
                if field_name := field.get('name'):
                    all_dmo_data[dmo_name]['fields'][field_name] = field.get('displayName', field_name)
    
    total_fields = sum(len(data['fields']) for data in all_dmo_data.values())
    logging.info(f"üó∫Ô∏è Mapeados {total_fields} campos em {len(all_dmo_data)} DMOs customizados (ap√≥s filtragem).")

    used_fields = set()

    # **MUDAN√áA**: An√°lise de Segmentos aprimorada
    for seg in segments_list:
        # Inspeciona tanto a defini√ß√£o antiga quanto a nova
        if criteria := seg.get('includeCriteria'): parse_json_from_string(criteria, used_fields)
        if criteria := seg.get('excludeCriteria'): parse_json_from_string(criteria, used_fields)
        if criteria := seg.get('filterDefinition'): parse_json_from_string(criteria, used_fields)
    logging.info(f"üîç Identificados {len(used_fields)} campos √∫nicos em Segmentos.")

    initial_count = len(used_fields)
    for act in detailed_activations:
        _recursive_find_fields(act, used_fields)
    logging.info(f"üîç Identificados {len(used_fields) - initial_count} campos adicionais em Ativa√ß√µes.")

    initial_count = len(used_fields)
    for ci in calculated_insights:
        _recursive_find_fields(ci.get('ciObject', ci), used_fields)
        for rel in ci.get('relationships', []):
            if rel.get('fromEntity'): used_fields.add(rel['fromEntity'])
    logging.info(f"üîç Identificados {len(used_fields) - initial_count} campos/objetos adicionais em Calculated Insights.")
    logging.info(f"Total de campos √∫nicos em uso: {len(used_fields)}")

    unused_field_results = []
    field_prefixes_to_exclude = ('ssot__', 'KQ_')
    specific_fields_to_exclude = {'DataSource__c', 'DataSourceObject__c', 'InternalOrganization__c'}

    if not all_dmo_data:
         logging.warning("\n‚ö†Ô∏è Nenhum DMO customizado (e n√£o-sistema) foi encontrado para auditar.")
         return

    for dmo_name, data in all_dmo_data.items():
        if dmo_name in used_fields: continue
        for field_api_name, field_display_name in data['fields'].items():
            if field_api_name not in used_fields:
                if not field_api_name.startswith(field_prefixes_to_exclude) and \
                   field_api_name not in specific_fields_to_exclude:
                    unused_field_results.append({
                        'DELETAR': 'NAO',
                        'DMO_DISPLAY_NAME': data['displayName'],
                        'DMO_API_NAME': dmo_name,
                        'FIELD_DISPLAY_NAME': field_display_name,
                        'FIELD_API_NAME': field_api_name, 
                        'REASON': 'N√£o utilizado em Segmentos, Ativa√ß√µes ou CIs'
                    })
    
    if not unused_field_results:
        logging.info("\nüéâ Nenhum campo √≥rf√£o (n√£o-sistema) encontrado. Todos os campos customizados est√£o em uso!")
    else:
        csv_file_path = 'audit_campos_dmo_nao_utilizados.csv'
        header = ['DELETAR', 'DMO_DISPLAY_NAME', 'DMO_API_NAME', 'FIELD_DISPLAY_NAME', 'FIELD_API_NAME', 'REASON']
        try:
            with open(csv_file_path, 'w', newline='', encoding='utf-8') as f:
                writer = csv.DictWriter(f, fieldnames=header)
                writer.writeheader()
                writer.writerows(unused_field_results)
            logging.info(f"\n‚úÖ Auditoria finalizada. {len(unused_field_results)} campos n√£o utilizados (e n√£o-sistema) encontrados.")
            logging.info(f"   Arquivo CSV gerado: {csv_file_path}")
        except IOError as e:
            logging.error(f"‚ùå Erro ao gravar o arquivo CSV: {e}")

if __name__ == "__main__":
    start_time = time.time()
    try:
        if os.name == 'nt':
            asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
        asyncio.run(audit_dmo_fields())
    except Exception as e:
        logging.error(f"Um erro inesperado durante o processo de auditoria: {e}", exc_info=True)
    finally:
        end_time = time.time()
        duration = end_time - start_time
        logging.info(f"\nTempo total de execu√ß√£o: {duration:.2f} segundos")